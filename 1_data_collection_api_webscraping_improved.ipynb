{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9884f9af",
   "metadata": {},
   "source": [
    "# SpaceX Falcon 9 — Data Collection (API + Web)\n",
    "\n",
    "**Purpose:** Collect SpaceX launch data using the public SpaceX REST API and an archived (static) Wikipedia table as a fallback for historical launch metadata.  \n",
    "\n",
    "This notebook is cleaned, documented, and structured for reproducible execution in **Google Colab** or **Jupyter Notebook**. Run the cells in order.  \n",
    "\n",
    "\n",
    "*Author: Sarthak Shandilya*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b767d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Setup: Install (if needed) & Imports\n",
    "# Run this cell first in Colab or Jupyter.\n",
    "# --------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# If running in Google Colab, uncomment the install line below.\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    COLAB = True\n",
    "except Exception:\n",
    "    COLAB = False\n",
    "\n",
    "if COLAB:\n",
    "    # install only when running in Colab\n",
    "    !pip install pandas requests beautifulsoup4 lxml html5lib plotly folium scikit-learn matplotlib seaborn nbformat --quiet\n",
    "\n",
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "print('Imports ready. Python version:', sys.version.split()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Functions: Data collection helpers (API + Wikipedia archived fallback)\n",
    "# --------------------\n",
    "\n",
    "def fetch_spacex_api(v4=True, timeout=15):\n",
    "    \"\"\"Fetch SpaceX launches. Returns a pandas DataFrame.\"\"\"\n",
    "    if v4:\n",
    "        url = 'https://api.spacexdata.com/v4/launches/past'\n",
    "    else:\n",
    "        url = 'https://api.spacexdata.com/v3/launches'\n",
    "    print(f'Fetching SpaceX API data from: {url}')\n",
    "    resp = requests.get(url, timeout=timeout)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    df = pd.json_normalize(data)\n",
    "    return df\n",
    "\n",
    "def fetch_wikipedia_falcon9(archived=True, timeout=20):\n",
    "    \"\"\"Attempt to fetch the Falcon 9 launch list from live Wikipedia; if the live page does not expose tables,\n",
    "    use an archived static snapshot (Wayback Machine) as a fallback. Returns a DataFrame with the main table.\"\"\"\n",
    "    live_url = 'https://en.wikipedia.org/wiki/List_of_Falcon_9_and_Falcon_Heavy_launches'\n",
    "    archive_url = 'https://web.archive.org/web/20240601000000/https://en.wikipedia.org/wiki/List_of_Falcon_9_and_Falcon_Heavy_launches'\n",
    "    urls_to_try = [live_url]\n",
    "    if archived:\n",
    "        urls_to_try.append(archive_url)\n",
    "    last_exception = None\n",
    "    for url in urls_to_try:\n",
    "        try:\n",
    "            print('Requesting:', url)\n",
    "            html = requests.get(url, timeout=timeout).text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            tables = soup.find_all('table')\n",
    "            if not tables:\n",
    "                raise ValueError('No <table> elements found on the page.')\n",
    "            largest = max(tables, key=lambda t: len(t.find_all('tr')))\n",
    "            df = pd.read_html(str(largest))[0]\n",
    "            print('Found table with shape', df.shape, 'from', url)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            last_exception = e\n",
    "            print('Attempt failed for', url, '->', repr(e))\n",
    "            continue\n",
    "    raise last_exception or ValueError('Failed to retrieve a table from Wikipedia.')\n",
    "\n",
    "# Note: do not call these functions automatically if you're offline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5361f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Execute data collection and save results\n",
    "# Run this cell to fetch data and persist CSV files in the notebook working directory.\n",
    "# --------------------\n",
    "from datetime import datetime\n",
    "out_dir = 'data'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# 1) API data\n",
    "try:\n",
    "    api_df = fetch_spacex_api(v4=True)\n",
    "    api_path = os.path.join(out_dir, 'spacex_api_raw.csv')\n",
    "    api_df.to_csv(api_path, index=False)\n",
    "    print('✅ SpaceX API data saved to', api_path, 'shape=', api_df.shape)\n",
    "except Exception as e:\n",
    "    print('✖ Failed to fetch SpaceX API data:', e)\n",
    "    api_df = None\n",
    "\n",
    "# 2) Wikipedia data (with archived fallback)\n",
    "try:\n",
    "    wiki_df = fetch_wikipedia_falcon9(archived=True)\n",
    "    wiki_path = os.path.join(out_dir, 'spacex_wikipedia_raw.csv')\n",
    "    wiki_df.to_csv(wiki_path, index=False)\n",
    "    print('✅ Wikipedia table saved to', wiki_path, 'shape=', wiki_df.shape)\n",
    "except Exception as e:\n",
    "    print('✖ Failed to fetch Wikipedia table:', e)\n",
    "    wiki_df = None\n",
    "\n",
    "# quick previews\n",
    "if api_df is not None:\n",
    "    display(api_df.head())\n",
    "if wiki_df is not None:\n",
    "    display(wiki_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dacb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Data Cleaning & Normalization (starter template)\n",
    "# --------------------\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_wiki_table(df):\n",
    "    df = df.copy()\n",
    "    df.columns = [re.sub('\\s+', '_', str(c).strip()).strip('_') for c in df.columns]\n",
    "    cols = df.columns.tolist()\n",
    "    mapping = {}\n",
    "    for c in cols:\n",
    "        lc = c.lower()\n",
    "        if 'date' in lc and 'time' in lc:\n",
    "            mapping[c] = 'Date'\n",
    "        elif 'date' in lc:\n",
    "            mapping[c] = 'Date'\n",
    "        elif 'rocket' in lc:\n",
    "            mapping[c] = 'Rocket'\n",
    "        elif 'launch' in lc and 'site' in lc:\n",
    "            mapping[c] = 'Launch_Site'\n",
    "        elif 'payload' in lc and ('mass' in lc or 'kg' in lc):\n",
    "            mapping[c] = 'Payload_Mass'\n",
    "        elif 'payload' in lc:\n",
    "            mapping[c] = 'Payload'\n",
    "        elif 'orbit' in lc:\n",
    "            mapping[c] = 'Orbit'\n",
    "        elif 'outcome' in lc or 'result' in lc:\n",
    "            mapping[c] = 'Outcome'\n",
    "        elif 'customer' in lc or 'operator' in lc:\n",
    "            mapping[c] = 'Customer'\n",
    "    df = df.rename(columns=mapping)\n",
    "    keep = [v for v in ['Date','Rocket','Launch_Site','Payload','Payload_Mass','Orbit','Outcome','Customer'] if v in df.columns]\n",
    "    df = df[keep]\n",
    "    if 'Date' in df.columns:\n",
    "        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "    if 'Payload_Mass' in df.columns:\n",
    "        df['Payload_Mass'] = df['Payload_Mass'].astype(str).str.replace('[^0-9\\.-]', '', regex=True)\n",
    "        df['Payload_Mass'] = pd.to_numeric(df['Payload_Mass'], errors='coerce')\n",
    "    if 'Outcome' in df.columns:\n",
    "        df['Outcome'] = df['Outcome'].astype(str).str.title()\n",
    "    return df\n",
    "\n",
    "# Example usage (run after wiki_df exists):\n",
    "# cleaned_wiki = clean_wiki_table(wiki_df)\n",
    "# cleaned_wiki.to_csv('data/spacex_wikipedia_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c634e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Starter EDA (visualizations)\n",
    "# --------------------\n",
    "\n",
    "import os\n",
    "api_path = os.path.join('data', 'spacex_api_raw.csv')\n",
    "wiki_path = os.path.join('data', 'spacex_wikipedia_raw.csv')\n",
    "\n",
    "if os.path.exists(api_path):\n",
    "    api_df = pd.read_csv(api_path)\n",
    "    print('API data shape:', api_df.shape)\n",
    "if os.path.exists(wiki_path):\n",
    "    wiki_df = pd.read_csv(wiki_path)\n",
    "    print('Wiki data shape:', wiki_df.shape)\n",
    "\n",
    "if 'date_utc' in api_df.columns:\n",
    "    api_df['date_utc'] = pd.to_datetime(api_df['date_utc'], errors='coerce')\n",
    "    api_df['year'] = api_df['date_utc'].dt.year\n",
    "    plt.figure(figsize=(10,4))\n",
    "    sns.countplot(x='year', data=api_df)\n",
    "    plt.title('Number of launches per year (API)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if 'success' in api_df.columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.countplot(x=api_df['success'].map({True:'Success', False:'Failure'}))\n",
    "    plt.title('Launch success / failure (API)')\n",
    "    plt.show()\n",
    "\n",
    "cleaned_wiki_path = os.path.join('data', 'spacex_wikipedia_cleaned.csv')\n",
    "if os.path.exists(cleaned_wiki_path):\n",
    "    cw = pd.read_csv(cleaned_wiki_path)\n",
    "    if 'Payload_Mass' in cw.columns:\n",
    "        plt.figure(figsize=(8,4))\n",
    "        sns.histplot(cw['Payload_Mass'].dropna(), bins=25, kde=True)\n",
    "        plt.title('Payload mass distribution (Wikipedia table)')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc9c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Modeling: Simple baseline classifier (example)\n",
    "# --------------------\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "if 'api_df' in globals() and 'date_utc' in api_df.columns and 'success' in api_df.columns:\n",
    "    api_df['year'] = pd.to_datetime(api_df['date_utc'], errors='coerce').dt.year.fillna(0).astype(int)\n",
    "    X = api_df[['year']].fillna(0)\n",
    "    y = api_df['success'].astype(int).fillna(0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    clf = DecisionTreeClassifier(random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('Baseline Decision Tree (year) accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "else:\n",
    "    print('Baseline model not executed: required columns missing (date_utc, success).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1394ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# Save cleaned data and final notes\n",
    "# --------------------\n",
    "\n",
    "if 'wiki_df' in globals():\n",
    "    try:\n",
    "        cleaned = clean_wiki_table(wiki_df)\n",
    "        cleaned_path = os.path.join('data', 'spacex_wikipedia_cleaned.csv')\n",
    "        cleaned.to_csv(cleaned_path, index=False)\n",
    "        print('Saved cleaned wikipedia table to', cleaned_path)\n",
    "    except Exception as e:\n",
    "        print('Could not clean or save wikipedia table:', e)\n",
    "\n",
    "print('\\nNext steps:')\n",
    "print('- Review cleaned CSVs in /data folder.')\n",
    "print('- Create separate notebooks for EDA, SQL queries, Folium map, and Dash dashboard as required by capstone rubric.')\n",
    "print('- Commit .ipynb, .py, cleaned CSVs, presentation PDF, and README.md to your GitHub repo.')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
